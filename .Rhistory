save.image()
# for(i in 1:length(urls)){
temp <- get(paste0("www.dr.dk/", urls[i]))
# for(i in 1:length(urls)){
temp <- get(paste0("www.dr.dk/", urls[i]))
# for(i in 1:length(urls)){
temp <- get(paste0("www.dr.dk", urls[i]))
# for(i in 1:length(urls)){
temp <- GET(paste0("www.dr.dk", urls[i]))
library(httr)
library(XML)
# for(i in 1:length(urls)){
temp <- GET(paste0("www.dr.dk", urls[i]))
temp <- htmlParse(temp)
urls[1]
urls[2]
text <- xpathSApply(temp, "//p")
text
temp <- htmlParse(temp, useInternalNodes = TRUE)
# for(i in 1:length(urls)){
temp <- GET(paste0("www.dr.dk", urls[i]))
?htmlParse()
text <- xpathSApply(temp, "//p", xmlValue)
# for(i in 1:length(urls)){
temp <- GET(paste0("www.dr.dk", urls[i]))
temp <- htmlParse(temp)
text <- xpathSApply(temp, "//p", xmlValue)
text
text
unlist(text)
paste(text)
text[1:2]
paste(text[1:2])
paste(unlist(text[1:2]))
typeof(text)
text[]
text[[]]
paste(text, collapse='')
message(paste(text, collapse=''))
message(paste(text, collapse=' '))
articles <- data.frame()
articles$url <- urls
articles <- data.frame(url = urls)
library(httr)
library(XML)
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-12-15")
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
temp <- GET(url)
temp <- htmlParse(temp)
as.vector(xpathSApply(temp, "//h3/a/@href")
)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
View(links)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), as.Date(date_i))
View(links)
as.Date(date_i)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-07-30")
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-07-30")
days <- as.integer(difftime(date_end, date_start))
articles <- NULL
for(i in 0:days){
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(c(links, articles))
message(paste("scraping", date_i, "-", length(urls), "urls collected"))
}
articles$url <- urls
remove(temp, links, i, days, url, date_i, url_base)
articles <- data.frame(url = urls, date=)
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-07-30")
days <- as.integer(difftime(date_end, date_start))
articles <- NULL
i = o
i = o0
i = 0
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(c(links, articles))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
for(i in 0:days){
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(c(links, articles))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
}
articles[1]
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
remove(temp, links, i, days, url, date_i, url_base)
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-07-30")
# Scraping URL's to news stories on dr.dk ---------------------------------
days <- as.integer(difftime(date_end, date_start))
articles <- NULL
for(i in 0:days){
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, articles))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
}
remove(temp, links, i, days, url, date_i, url_base)
View(articles)
articles <- data.frame(url = articles$V1, date=articles$date_i)
articles[1]
articles[2]
articles[2,2]
date_end+1
cbind(date_end)
cbind(as.Date(date_end))
cbind(as.Date.string(date_end))
cbind(format(date_end, "%d%m%Y"))
cbind(format(date_end, "%d-%m-%Y"))
articles <- data.frame(url = articles[1], date=articles$date_i)
articles$v1
articles$V1
temp <- as.data.frame(articles)
?data.frame
# Settings ----------------------------------------------------------------
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-07-30")
# Scraping URL's to news stories on dr.dk ---------------------------------
days <- as.integer(difftime(date_end, date_start))
articles <- NULL
for(i in 0:days){
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, format(articles, "%d-%m-%Y")))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
}
remove(temp, links, i, days, url, date_i, url_base)
# Creating data frame -----------------------------------------------------
articles <- data.frame(articles)
names(articles) <- c("url", "date")
i = 1
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
temp <- GET(url)
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-07-30")
days <- as.integer(difftime(date_end, date_start))
articles <- NULL
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, format(articles, "%d-%m-%Y")))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
i = 2
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, format(articles, "%d-%m-%Y")))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
library(httr)
library(XML)
# Settings ----------------------------------------------------------------
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-07-30")
# Scraping URL's to news stories on dr.dk ---------------------------------
days <- as.integer(difftime(date_end, date_start))
articles <- NULL
for(i in 0:days){
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, format(articles, "%d-%m-%Y")))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
}
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
temp <- GET(url)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
temp <- htmlParse(temp)
articles <- unique(rbind(links, format(articles, "%d-%m-%Y")))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-07-30")
days <- as.integer(difftime(date_end, date_start))
articles <- NULL
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
i = 0
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, format(articles, "%d-%m-%Y")))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
i = i+1
date_i <- date_start + i
url <- paste0(url_base, format(date_start+i, "%d%m%Y"))
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, format(articles, "%d-%m-%Y")))
# Settings ----------------------------------------------------------------
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-07-30")
# Scraping URL's to news stories on dr.dk ---------------------------------
days <- as.integer(difftime(date_end, date_start))
articles <- NULL
for(i in 0:days){
date_i <- format(date_start+i, "%d%m%Y")
url <- paste0(url_base, date_i)
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, articles))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
i = i+1
}
remove(temp, links, i, days, url, date_i, url_base)
articles <- data.frame(articles)
names(articles) <- c("url", "date")
View(articles)
i = 1
# for(i in 1:length(urls)){
temp <- GET(paste0("www.dr.dk", articles$url[i]))
temp <- htmlParse(temp)
text <- xpathSApply(temp, "//p", xmlValue)
text <- paste(text, collapse=" ")
articles$text[i] <- text
View(articles)
articles$text <- NULL
View(articles)
articles$date <- format(articles$date, "%Y-%m-%d")
format(articles$date, "%Y-%m-%d")
format(articles$date, "%YY-%m-%d")
format(articles$date, format="%YY-%m-%d")
format(articles$date, format="%Y-%m-%d")
?format
articles$text[i,] <- text
articles$text[i] <- text
View(articles)
articles$text <- NULL
articles$text <- NA
articles <- NULL
remove(articles)
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-07-30")
days <- as.integer(difftime(date_end, date_start))
for(i in 0:days){
date_i <- format(date_start+i, "%d%m%Y")
url <- paste0(url_base, date_i)
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, articles))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
i = i+1
}
articles <- NULL
for(i in 0:days){
date_i <- format(date_start+i, "%d%m%Y")
url <- paste0(url_base, date_i)
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, articles))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
i = i+1
}
remove(temp, links, i, days, url, date_i, url_base)
articles <- data.frame(articles)
names(articles) <- c("url", "date")
articles$text <- NA
for(i in 1:length(urls)){
temp <- GET(paste0("www.dr.dk", articles$url[i]))
temp <- htmlParse(temp)
text <- xpathSApply(temp, "//p", xmlValue)
text <- paste(text, collapse=" ")
articles$text[i] <- text
}
for(i in 1:nrow(articles)){
temp <- GET(paste0("www.dr.dk", articles$url[i]))
temp <- htmlParse(temp)
text <- xpathSApply(temp, "//p", xmlValue)
text <- paste(text, collapse=" ")
articles$text[i] <- text
}
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-07-30")
# Scraping URL's to news stories on dr.dk ---------------------------------
days <- as.integer(difftime(date_end, date_start))
articles <- NULL
for(i in 0:days){
date_i <- format(date_start+i, "%d%m%Y")
url <- paste0(url_base, date_i)
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, articles))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
i = i+1
}
remove(temp, links, i, days, url, date_i, url_base)
# Creating data frame -----------------------------------------------------
articles <- data.frame(articles)
names(articles) <- c("url", "date")
# Scraping text for each article ------------------------------------------
articles$text <- NA
for(i in 1:nrow(articles)){
temp <- GET(paste0("www.dr.dk", articles$url[i]))
temp <- htmlParse(temp)
text <- xpathSApply(temp, "//p", xmlValue)
text <- paste(text, collapse=" ")
articles$text[i] <- text
message(paste("scraping aritcle", i, "of", nrow(articles) ))
}
226/60
# Settings ----------------------------------------------------------------
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-07-30")
# Scraping URL's to news stories on dr.dk ---------------------------------
days <- as.integer(difftime(date_end, date_start))
articles <- NULL
for(i in 0:days){
date_i <- format(date_start+i, "%d%m%Y")
url <- paste0(url_base, date_i)
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, articles))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
i = i+1
}
remove(temp, links, i, days, url, date_i, url_base)
# Creating data frame -----------------------------------------------------
articles <- data.frame(articles)
names(articles) <- c("url", "date")
# Scraping text for each article ------------------------------------------
articles$text <- NA
for(i in 1:nrow(articles)){
temp <- GET(paste0("www.dr.dk", articles$url[i]))
temp <- htmlParse(temp)
text <- xpathSApply(temp, "//p", xmlValue)
text <- paste(text, collapse=" ")
articles$text[i] <- text
message(paste("scraping aritcle", i, "of", nrow(articles) ))
Sys.sleep(2)
}
articles[226]
articles[226,]
articles[227,]
error
1+o
try(1+o)
1+1
1+a
1+1
1+a
1+2
for(i in 1:10){
1+1
1+a
1+2
}
for(i in 1:10){
1+1
try(1+a)
1+2
}
?try
fail <- c(1)
remove(fail)
fail <- c(i, 1)
remove(i)
fail <- c(i, 1)
# Libraries ---------------------------------------------------------------
library(httr)
library(XML)
# Settings ----------------------------------------------------------------
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-07-04")
# Scraping URL's to news stories on dr.dk ---------------------------------
days <- as.integer(difftime(date_end, date_start))
articles <- NULL
for(i in 0:days){
date_i <- format(date_start+i, "%d%m%Y")
url <- paste0(url_base, date_i)
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, articles))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
i = i+1
}
remove(temp, links, i, days, url, date_i, url_base)
# Creating data frame -----------------------------------------------------
articles <- data.frame(articles)
names(articles) <- c("url", "date")
# Scraping text for each article ------------------------------------------
articles$text <- NA
for(i in 1:nrow(articles)){
temp <- GET(paste0("www.dr.dk", articles$url[i]))
temp <- htmlParse(temp)
text <- xpathSApply(temp, "//p", xmlValue)
text <- paste(text, collapse=" ")
articles$text[i] <- text
message(paste("scraping aritcle", i, "of", nrow(articles) ))
}
View(articles)
articles[171,1]
grepl(articles$text[1])
grepl(" ", articles$text[1])
grep(" ", articles$text[1])
?grep
articles$text[1]
?strsplit
strsplit(articles$text[1])
strsplit(articles$text[1], split=" ")
strsplit(articles$text[1], split="[A-z]\\W+")
lenthg(strsplit(articles$text[1], split="[A-z]\\W+")
length(strsplit(articles$text[1], split="[A-z]\\W+"))
strsplit(articles$text[1], split="[A-z]\\W+")
abe <- strsplit(articles$text[1], split="[A-z]\\W+")
abe <- nrow(strsplit(articles$text[1], split="[A-z]\\W+"))
lengths(strsplit(articles$text[1], "\\W+"))
lengths(strsplit(articles$text, "\\W+"))
articles$words <- lengths(strsplit(articles$text, "\\W+"))
View(articles)
url_base <- "http://www.dr.dk/nyheder/allenyheder/"
date_start <- as.Date("2018-07-01")
date_end <- as.Date("2018-12-15")
# Scraping URL's to news stories on dr.dk ---------------------------------
days <- as.integer(difftime(date_end, date_start))
articles <- NULL
for(i in 0:days){
date_i <- format(date_start+i, "%d%m%Y")
url <- paste0(url_base, date_i)
temp <- GET(url)
temp <- htmlParse(temp)
links <- cbind(as.vector(xpathSApply(temp, "//h3/a/@href")), date_i)
articles <- unique(rbind(links, articles))
message(paste("scraping", date_i, "-", length(articles), "urls collected"))
}
remove(temp, links, i, days, url, date_i, url_base)
# Creating data frame -----------------------------------------------------
articles <- data.frame(articles)
names(articles) <- c("url", "date")
# Scraping text for each article ------------------------------------------
articles$text <- NA
for(i in 1:nrow(articles)){
temp <- GET(paste0("www.dr.dk", articles$url[i]))
temp <- htmlParse(temp)
text <- xpathSApply(temp, "//p", xmlValue)
text <- paste(text, collapse=" ")
articles$text[i] <- text
message(paste("scraping aritcle", i, "of", nrow(articles) ))
}
View(articles)
articles$words <- lengths(strsplit(articles$text, "\\W+"))
hist(articles$words)
sum(is.na(articles$text))
sum(!is.na(articles$text))
plot(articles$words)
getwd()
setwd(".")
getwd()
setwd("..")
getwd()
setwd("/functions")
setwd("\functions")
save(articles, file="\Data\Articles.rda")
save(articles, file="/Data/Articles.rda")
dir()
save(articles, file="/data/articles.rda")
?save
save(articles, file="/data")
save.image()
